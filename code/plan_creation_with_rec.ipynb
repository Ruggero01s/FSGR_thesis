{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "667eec6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from os.path import  join, isdir\n",
    "from plan import Plan\n",
    "from action import Action\n",
    "from utils import load_from_folder\n",
    "from multiprocess import Pool\n",
    "import random\n",
    "from logging import exception\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b7d761e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain dir: ../datasets/logistics/optimal_plans/dictionaries_and_plans/\n"
     ]
    }
   ],
   "source": [
    "save_dir = './new_plans/'\n",
    "data_base_dir = '../datasets/'\n",
    "domain = 'logistics'\n",
    "results_dir = f\"{save_dir}/{domain}/\"   \n",
    "source_dir = f\"{join(data_base_dir, domain)}/optimal_plans/dictionaries_and_plans/\" \n",
    "print('Domain dir:', source_dir)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "plans_to_process = 1000 # number of plans to process\n",
    "versions_per_plan = 6 # number of versions per each plan\n",
    "number_of_goals = 4 # number of goals per each new plan\n",
    "test = False # test will process only 3 plans\n",
    "rec_classes = [[0,0.1], [0.1,0.2], [0.2,0.3], [0.3,0.4], [0.4,0.5], [0.5,0.6], [0.6,0.7], [0.7,0.8], [0.8,0.9], [0.9,1]] # classes of recognizability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fe052be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to clear the results directory\n",
    "for folder in os.listdir(results_dir):\n",
    "    #remove directory if it exists along with its content\n",
    "    if isdir(join(results_dir, folder)):\n",
    "        shutil.rmtree(join(results_dir, folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ac5476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plans loaded from ../datasets/logistics/optimal_plans/dictionaries_and_plans/\n",
      "Plans: 47769\n"
     ]
    }
   ],
   "source": [
    "#loading the plans\n",
    "plans = load_from_folder(source_dir,[\"plans\"])[0]\n",
    "print(f\"Plans: {len(plans)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "34a041db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recognizability(current_goal_state, goal_state_list):\n",
    "    \"\"\"\n",
    "    Compute the difficulty of a plan.\n",
    "    :param current_goal_state: The goal state for which we calculate the recognizability.\n",
    "    :param goal_state_list: The list of goal states to use for the computation.\n",
    "    :return: The recognizability of the plan.\n",
    "    \"\"\"\n",
    "    \n",
    "    #the current goal state must be in the goal state list\n",
    "    if current_goal_state not in goal_state_list:\n",
    "        raise ValueError(f\"current_goal_state {current_goal_state} must be included in goal_state_list {goal_state_list}\")\n",
    "    \n",
    "    #min and max to use for normalization\n",
    "    #min recognizability is when all the fluent in current are present in each goal state in the goal state list\n",
    "    #the formula becomes 1+1+1+...+1 = len(goal_state_list)\n",
    "    #max recognizability is when all the fluent in current are not present anywhere in the goal state list\n",
    "    rec_when_least_recognisable  = 1*len(current_goal_state)\n",
    "    rec_when_most_recognisable  = 1/(len(goal_state_list)) * len(current_goal_state)\n",
    "    \n",
    "    # print(f\"rec_when_least_recognisable: {rec_when_least_recognisable}\")\n",
    "    # print(f\"rec_when_most_recognisable: {rec_when_most_recognisable}\")\n",
    "    \n",
    "    sum = 0\n",
    "    #need to count how many times the current goal fluent is in the goal state list\n",
    "    for current_goal_fluent in current_goal_state:\n",
    "        count = 0\n",
    "        for goal_state in goal_state_list:\n",
    "            for goal_fluent in goal_state:       \n",
    "                if current_goal_fluent==goal_fluent:\n",
    "                    count += 1\n",
    "                    break\n",
    "        sum += 1/count\n",
    "    \n",
    "    #print(f\"Unscaled recognizability: {sum}\")\n",
    "    \n",
    "    #normalize the recognizability \n",
    "    recognizability = (sum-rec_when_most_recognisable) / (rec_when_least_recognisable-rec_when_most_recognisable)\n",
    "    \n",
    "    return round(recognizability, 4)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ceab2950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec_when_least_recognisable: 3\n",
      "rec_when_most_recognisable: 1.0\n",
      "0.75\n"
     ]
    }
   ],
   "source": [
    "#test compute_recognizability\n",
    "goal_state_list = [[\"a\", \"b\", \"c\"], \n",
    "                   [\"a\", \"e\", \"f\"], \n",
    "                   [\"g\", \"h\", \"i\"], ]\n",
    "print(compute_recognizability(goal_state_list[0], goal_state_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "879de6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute difficulty testing\n",
    "# current_goal_state = [6, 5, 7, 9]\n",
    "# goal_state_list = [[1, 2, 3, 4], \n",
    "#                    [1, 2, 3, 4], \n",
    "#                    [1, 2, 3, 4], \n",
    "#                    [1, 2, 3, 4], \n",
    "#                    [1, 2, 3, 4]]\n",
    "# goal_state_list.append(current_goal_state)\n",
    "# rec = compute_recognizability(current_goal_state, goal_state_list)\n",
    "# print(f\"recognizability : {rec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d502f195",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #?tried to have a way to generate the goal state list directly with a given recognizability\n",
    "    #?kinda works, but approximation is too large, and it is unncessarily complex\n",
    "    \n",
    "    #todo check code again, gpt helped with it\n",
    "    \n",
    "# import math\n",
    "\n",
    "# def approx_counts(num_of_fluents: int, num_of_states: int, r: float) -> list:\n",
    "#     \"\"\"\n",
    "#     Return a list of n integer counts in [1..m] whose reciprocals sum\n",
    "#     approximately to S = n/m + r * n*(m-1)/m.\n",
    "\n",
    "#     Strategy:\n",
    "#       1. Compute the “continuous” count c = m / (1 + r*(m-1)).\n",
    "#       2. If c is (nearly) integer, just return [round(c)]*n.\n",
    "#       3. Otherwise let b = floor(c), B = b + 1, and solve for x:\n",
    "#             x*(1/b) + (n-x)*(1/B) = S\n",
    "#          => x = (n/B - S) / (1/B - 1/b)\n",
    "#       4. Round x to the nearest integer; assign x entries = b, (n-x) = B.\n",
    "#       5. Clip to [0..n] and, if desired, do a tiny local tweak\n",
    "#          (e.g. move 1 count up/down) to reduce the residual error.\n",
    "#     \"\"\"\n",
    "#     # 1) target unscaled sum\n",
    "#     S = (num_of_fluents/num_of_states) + r * (num_of_fluents * (num_of_states-1) / num_of_states)\n",
    "\n",
    "#     # 2) continuous ideal count\n",
    "#     c = num_of_states / (1 + r*(num_of_states-1))\n",
    "#     c_round = round(c)\n",
    "#     # if it’s essentially integral, use it\n",
    "#     if abs(c_round - c) < 1e-6 or c_round in (1, num_of_states):\n",
    "#         return [c_round] * num_of_fluents\n",
    "\n",
    "#     # 3) floor / ceil\n",
    "#     b = math.floor(c)\n",
    "#     B = b + 1\n",
    "\n",
    "#     # solve x*(1/b) + (n-x)*(1/B) = S\n",
    "#     #   => x = (n/B - S) / (1/B - 1/b)\n",
    "#     denom = (1/B - 1/b)\n",
    "#     if abs(denom) < 1e-8:\n",
    "#         # degenerate; fallback to uniform\n",
    "#         return [c_round] * num_of_fluents\n",
    "\n",
    "#     x_real = (num_of_fluents/B - S) / denom\n",
    "#     x = int(round(x_real))\n",
    "\n",
    "#     # 4) clip and build\n",
    "#     x = max(0, min(num_of_fluents, x))\n",
    "#     counts = [b]*x + [B]*(num_of_fluents - x)\n",
    "\n",
    "#     # 5) (optional) tiny local corrections\n",
    "#     # compute residual error\n",
    "#     current_sum = sum(1/ci for ci in counts)\n",
    "#     # if we’re off by more than, say, 1/n, try one adjustment\n",
    "#     if abs(current_sum - S) > 1e-3:\n",
    "#         # if sum too small, we need to increase it ⇒ lower some ci by 1\n",
    "#         if current_sum < S:\n",
    "#             # find an index with ci > 1 and decrement it\n",
    "#             for i in range(num_of_fluents):\n",
    "#                 if counts[i] > 1:\n",
    "#                     counts[i] -= 1\n",
    "#                     break\n",
    "#         else:\n",
    "#             # sum too big ⇒ decrement sum ⇒ increase some ci by 1\n",
    "#             for i in range(num_of_fluents):\n",
    "#                 if counts[i] < num_of_states:\n",
    "#                     counts[i] += 1\n",
    "#                     break\n",
    "\n",
    "#     return counts\n",
    "\n",
    "\n",
    "\n",
    "# def build_goal_list(current, goal_set_list_size, counts, fillers=['a','b','c','d','e']):\n",
    "#     num_of_goal_fluents = len(current)\n",
    "#     # initialize empty slots\n",
    "#     slots = [ [] for _ in range(goal_set_list_size) ]\n",
    "#     # for each fluent, choose which rows it goes in\n",
    "#     for fluent, c in zip(current, counts):\n",
    "#         rows = random.sample(range(goal_set_list_size), c)\n",
    "#         for r in rows:\n",
    "#             slots[r].append(fluent)\n",
    "#     # fill the rest with distractors\n",
    "#     all_distractors = ['a','b','c','d','e']  # pool of non-current fluents\n",
    "#     for r in range(goal_set_list_size):\n",
    "#         while len(slots[r]) < num_of_goal_fluents:\n",
    "#             slots[r].append(random.choice(all_distractors))\n",
    "#         random.shuffle(slots[r])\n",
    "#     return slots\n",
    "\n",
    "# # Example usage:\n",
    "# current = [6,5,7,9]\n",
    "# goal_set_list_size = 6\n",
    "# # counts chosen by solving sum(1/ci)=S (approximated)\n",
    "# counts = approx_counts(len(current), goal_set_list_size, 0.47)\n",
    "# print(\"Counts:\", counts)\n",
    "# goal_list = build_goal_list(current, goal_set_list_size, counts)\n",
    "# print(\"Goal list:\")\n",
    "# for i, g in enumerate(goal_list):\n",
    "#     print(f\"Goal {i}: {g}\")\n",
    "# rec = compute_recognizability(current, goal_list)\n",
    "# print(f\"recognizability : {rec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a315544",
   "metadata": {},
   "outputs": [],
   "source": [
    " #todo subject to change depending on how we want output\n",
    "def write_and_save_versions(plan, goal_state_list, obj_set_dict={}, rec_class=[0,1]):\n",
    "    \"\"\"Write the plan and its versions to files.\n",
    "    :param plan: The plan to write.\n",
    "    :param goal_state_list: The list of goal states, each state will produce a different version, at index 0 should be the original plan.\n",
    "    :param obj_set_dict: The dictionary of objects.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "        \n",
    "    for i, goal_state in enumerate(goal_state_list):\n",
    "        \n",
    "        #extract plan name with regex\n",
    "        name = re.search(r\"(p\\d+)(?=\\.)\", plan.plan_name).group(1)\n",
    "        \n",
    "        #definition\n",
    "        new_problem = \"\"\n",
    "        # if i==0:\n",
    "        #     new_problem += f\";;(;metadata (recognizability:{round(compute_recognizability(goal_state_list[0], goal_state_list),2)})\\n\"\n",
    "        \n",
    "        new_problem += f\"(define (problem {domain}_{name}_{i})\\n(:domain {domain})\\n(:objects\\n\\t\"\n",
    "    \n",
    "        \n",
    "        #objects in a dict format, {type: obj_set}\n",
    "        for type, obj_set in obj_set_dict.items():\n",
    "            if len(obj_set) > 0:\n",
    "                for obj in obj_set:\n",
    "                    new_problem += f\"{obj} \"\n",
    "                new_problem += f\"- {type}\\n\\t\"\n",
    "        new_problem += f\")\\n\"\n",
    "        \n",
    "        #initial state\n",
    "        new_problem += f\"(:init\\n\"\n",
    "        for fluent in plan.initial_state:\n",
    "            new_problem += f\"\\t{fluent}\\n\"\n",
    "        new_problem += f\")\\n\"\n",
    "        \n",
    "        #goal state\n",
    "        new_problem += f\"(:goal (and\\n\"\n",
    "        for goal in goal_state:\n",
    "            new_problem += f\"\\t{goal}\\n\"\n",
    "        new_problem += f\"))\\n)\"\n",
    "        #print(new_problem + \"\\n\\n\")\n",
    "        \n",
    "        #save the new problem in a file\n",
    "        #naming convention is {current plan name_version number.pddl}, _0 is the original plan\n",
    "        new_problem_dir = f\"{results_dir}/{name}/\"\n",
    "        class_dir = f\"{results_dir}/{name}/{rec_class[0]}_{rec_class[1]}/\"\n",
    "        os.makedirs(new_problem_dir, exist_ok=True)\n",
    "        os.makedirs(class_dir, exist_ok=True)\n",
    "        new_problem_file = f\"{class_dir}/{name}_{i}.pddl\"\n",
    "        with open(new_problem_file, \"w\") as f:\n",
    "            f.write(new_problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04fbf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    " #todo subject to change depending on how we want output\n",
    "\n",
    "def write_original_plan(plan, goal_state, obj_set_dict):\n",
    "    \n",
    "    #extract plan name with regex\n",
    "    name = re.search(r\"(p\\d+)(?=\\.)\", plan.plan_name).group(1)\n",
    "    \n",
    "    #definition\n",
    "    new_problem = \"\"\n",
    "    \n",
    "    new_problem += f\"(define (problem {domain}_{name}_og)\\n(:domain {domain})\\n(:objects\\n\\t\"\n",
    "\n",
    "    \n",
    "    #objects in a dict format, {type: obj_set}\n",
    "    for type, obj_set in obj_set_dict.items():\n",
    "        if len(obj_set) > 0:\n",
    "            for obj in obj_set:\n",
    "                new_problem += f\"{obj} \"\n",
    "            new_problem += f\"- {type}\\n\\t\"\n",
    "    new_problem += f\")\\n\"\n",
    "    \n",
    "    #initial state\n",
    "    new_problem += f\"(:init\\n\"\n",
    "    for fluent in plan.initial_state:\n",
    "        new_problem += f\"\\t{fluent}\\n\"\n",
    "    new_problem += f\")\\n\"\n",
    "    \n",
    "    #goal state\n",
    "    new_problem += f\"(:goal (and\\n\"\n",
    "    for goal in goal_state:\n",
    "        new_problem += f\"\\t{goal}\\n\"\n",
    "    new_problem += f\"))\\n)\"\n",
    "    #print(new_problem + \"\\n\\n\")\n",
    "    \n",
    "    #save the new problem in a file\n",
    "    #naming convention is {current plan name_version number.pddl}\n",
    "    new_problem_dir = f\"{results_dir}/{name}/\"\n",
    "    os.makedirs(new_problem_dir, exist_ok=True)\n",
    "    new_problem_file = f\"{new_problem_dir}/{name}_og.pddl\"\n",
    "    with open(new_problem_file, \"w\") as f:\n",
    "        f.write(new_problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f04baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_goal_state_list(package_for_goal_set, pos_for_goal_set, number_of_goals, versions_per_plan):\n",
    "    \"\"\"Create a list of goal states.\n",
    "    :param package_for_goal_set: The set of packages to use for the goal state.\n",
    "    :param pos_for_goal_set: The set of positions to use for the goal state.\n",
    "    :param number_of_goals: The number of goals to generate.\n",
    "    :param versions_per_plan: The number of versions to generate.\n",
    "    :return: A list of goal states.\n",
    "    \"\"\"\t\n",
    "    goal_state_list = []\n",
    "    for i in range(0, versions_per_plan + 1):\n",
    "        # generate a random goal state\n",
    "        goal_state = generate_goal_state(package_for_goal_set, pos_for_goal_set, number_of_goals)\n",
    "            \n",
    "        goal_state_list.append(goal_state)\n",
    "        \n",
    "    for i, goal_state in enumerate(goal_state_list):\n",
    "        #convert the sets to a list\n",
    "        goal_state_list[i] = list(goal_state)\n",
    "        \n",
    "    return goal_state_list\n",
    "        \n",
    "def generate_goal_state(package_for_goal_set, pos_for_goal_set, number_of_goals):\n",
    "    \"\"\"Generate a random goal state.\t\n",
    "    :param package_for_goal_set: The set of packages to use for the goal state.\n",
    "    :param pos_for_goal_set: The set of positions to use for the goal state.\n",
    "    :param number_of_goals: The number of fluents to generate for the goal state.\n",
    "    :return: A set of fluents representing a goal state.\n",
    "    \"\"\"\n",
    "    goal_state = set()\n",
    "    package_for_goal_set_copy = package_for_goal_set.copy()\n",
    "    pos_for_goal_set_copy = pos_for_goal_set.copy()\n",
    "    for _ in range(number_of_goals):\n",
    "        random_package = random.choice(list(package_for_goal_set_copy))\n",
    "        package_for_goal_set_copy.remove(random_package)\n",
    "        random_pos = random.choice(list(pos_for_goal_set_copy))\n",
    "        #pos_for_goal_set_copy.remove(random_pos)\n",
    "        goal_state.add(f\"at {random_package} {random_pos}\") \n",
    "    return goal_state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f25e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to calcculate how precise the generation is\n",
    "global_counter = 0\n",
    "\n",
    "running_sum_rec_error = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89119559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_consistency(goal_state, prefix=\"obj\"):\n",
    "    \"\"\"\t\n",
    "    Check if the goal state is consistent by checking if it has same obj used more than once.\n",
    "    :param goal_state: The goal state to check.\n",
    "    :return: True if the goal state is consistent, False otherwise.\n",
    "    \"\"\"\n",
    "    objects = []\n",
    "    for fluent in goal_state:\n",
    "        obj = re.search(rf\"{prefix}\\d+\", fluent)\n",
    "        objects.append(obj.group(0))\n",
    "    return len(objects) == len(set(objects))\n",
    "        \n",
    "\n",
    "def check_if_fluent_is_usable(fluent_to_add, goal_state):\n",
    "    \"\"\"\n",
    "    Check if the fluent to add is usable in the goal state.\n",
    "    This means checking if fluent is not already in the goal state and if the object in fluent is not already in the goal state.\n",
    "    :param fluent_to_add: The fluent to add.\n",
    "    :param goal_state: The goal state to check.\n",
    "    :return: True if the fluent to add is usable, False otherwise.\n",
    "    \"\"\"\n",
    "    #check if goal state is a list of strings\n",
    "    if isinstance(fluent_to_add, list):\n",
    "        raise ValueError(f\"fluent to add is a list: {fluent_to_add}\")\n",
    "    for fluent in goal_state:\n",
    "        if fluent == fluent_to_add:\n",
    "            return False\n",
    "        elif check_same_object_in_fluents(fluent_to_add, fluent, prefix=\"obj\"):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "#todo fix to introduce prefix\n",
    "def check_same_object_in_fluents(fluent1, fluent2, prefix=\"obj\"):\n",
    "    \"\"\"\n",
    "    Check if the object in fluent1 is in fluent2.\n",
    "    :return: True if the object in fluent1 is in fluent2, False otherwise.\n",
    "    \"\"\"\n",
    "    #extract the object from the fluent using regex\n",
    "    obj1 = re.search(rf\"{prefix}\\d+\", fluent1).group(0)\n",
    "    obj2 = re.search(rf\"{prefix}\\d+\", fluent2).group(0)\n",
    "\n",
    "    if obj1 == obj2:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def adapt_goal_state_list_to_recognizability(base_goal_state, goal_state_list, \n",
    "                                             package_for_goal_set, pos_for_goal_set, \n",
    "                                             number_of_goals, rec_target=[0.1, 0.2], \n",
    "                                             randomness_patience=5, regeneration_patience=3):\n",
    "    \"\"\"\n",
    "    Adapt the goal state list to the recognizability. \n",
    "    This is done by swapping fluents between the base goal state and the goal state list.\n",
    "    If we have to reduce recognisability, we swap a random fluent in the goal state list with a fluent from the base goal state.\n",
    "    If we have to increase recognisability, we swap a fluent that is in the base goal state and in also in a goal in goal state list with a random possible fluent.\n",
    "    This does not guarantee that the recognizability will be in the target range, but it will be close.\n",
    "    It can happen that a goal state will be stuck in a local minimum, so we regenerate it to try staring from another point.\n",
    "    We keep track of which regenerations we have done and if we reach the patience limit, we will use the one that is closest to the target recognizability.\n",
    "    :param base_goal_state: The base goal state.\n",
    "    :param goal_state_list: The list of goal states to use to compute recognizability, without base goal state.\n",
    "    :param randomness_patience: The number of times we can try to adapt the goal state list before regenerating it. This will usually exhaust if we have many states that are stuck in a local minimum. If we reach this limit, we will regenerate the next stuck goal state.\n",
    "    :param regeneration_patience: The number of times we can regenerate a goal state before giving up.\n",
    "    :param package_for_goal_set: The set of packages to use for the goal state.\n",
    "    :param pos_for_goal_set: The set of positions to use for the goal state.\n",
    "    :param number_of_goals: The number of goals to generate.\n",
    "    :param rec_target: The target range of recognizability.\n",
    "    :return: The adapted goal state list.\n",
    "    \"\"\"\n",
    "    #variables to keep track of errors\n",
    "    global global_counter\n",
    "    global running_sum_rec_error\n",
    "    \n",
    "    \n",
    "    randomness_patience_constant = randomness_patience\n",
    "    \n",
    "    #to keep track of the goal states that we have regenerated\n",
    "    goal_regeneration_dict = {}\n",
    "    \n",
    "    #we identify all the fluents that are in the goal state list    \n",
    "    all_goal_fluents = []\n",
    "    for goal_state in goal_state_list:\n",
    "        for fluent in goal_state:\n",
    "            if fluent not in all_goal_fluents:\n",
    "                all_goal_fluents.append(fluent)\n",
    "    \n",
    "    #we identify all the fluents that are not in the base goal state but are in the goal state list\n",
    "    non_base_goal_fluents = []\n",
    "    for fluent in all_goal_fluents:\n",
    "        if fluent not in base_goal_state:\n",
    "            non_base_goal_fluents.append(fluent)\n",
    "    \n",
    "    #starting recognizability\n",
    "    running_recognizability = compute_recognizability(base_goal_state, [base_goal_state] + goal_state_list)\n",
    "    #print(f\"Starting recognizability: {running_recognizability}, range is {recognizability}\")\n",
    "    \n",
    "    #while the recognizability is not in the target range, we will keep adapting the goal state list\n",
    "    while running_recognizability < rec_target[0] or running_recognizability > rec_target[1]:\n",
    "        #print(f\"Running recognizability start of step: {running_recognizability}, range is {recognizability}\")\n",
    "        \n",
    "        #choose a random goal state from the list\n",
    "        goal_state = random.choice(goal_state_list)\n",
    "        \n",
    "        if running_recognizability > rec_target[1]:\n",
    "            #rec too high: find a goal state in goal_state_list that has a fluent that is not in the base goal state\n",
    "            #swap it with one from usable_base_goal_fluents                \n",
    "            \n",
    "            #builds list of fluents that are in the base goal state but not in this goal state and don't introduce inconsistencies\n",
    "            usable_base_goal_fluents = []\n",
    "            for fluent in base_goal_state:  \n",
    "                if check_if_fluent_is_usable(fluent, goal_state):\n",
    "                    usable_base_goal_fluents.append(fluent)\n",
    "                            \n",
    "            if len(usable_base_goal_fluents) > 0:\n",
    "                #swap a fluent from the goal state with one from the base goal state\n",
    "                fluent_to_swap = random.choice(usable_base_goal_fluents)\n",
    "                \n",
    "                #builds list of fluents that are in the goal state but not in the base goal state\n",
    "                candidates_list = []\n",
    "                for fluent in goal_state:\n",
    "                    if fluent not in base_goal_state:\n",
    "                        candidates_list.append(fluent)\n",
    "                \n",
    "                #if there is at least one fluent to swap   \n",
    "                if len(candidates_list) > 0:\n",
    "                    # debug_goal_state = goal_state.copy() #debug\n",
    "                    random_fluent = random.choice(candidates_list)\n",
    "                    \n",
    "                    # before = check_consistency(goal_state) #debug\n",
    "                    #we make the swap\n",
    "                    goal_state[goal_state.index(random_fluent)] = fluent_to_swap\n",
    "                    # after = check_consistency(goal_state) #debug\n",
    "                    # if before == True and after == False: #debug\n",
    "                    #     print(f\"|>|Goal state is not consistent: old goal state{debug_goal_state}\\n\\t base_goal_state: {base_goal_state},\\n\\t fluent_to_swap: {fluent_to_swap},\\n\\t random_fluent: {random_fluent}, \\n\\t usable_base_goal_fluents: {usable_base_goal_fluents}\\n\\n\") #debug\n",
    "                                    \n",
    "        elif running_recognizability < rec_target[0]:\n",
    "            #rec too low: choose a goal state that has a fluent from base goal and swap it with a random one\n",
    "            \n",
    "            #builds list of fluents that are in the base goal state and also in this goal state\n",
    "            present_base_goal_fluents = []\n",
    "            for fluent in base_goal_state:\n",
    "                if fluent in goal_state:\n",
    "                    present_base_goal_fluents.append(fluent)\n",
    "            \n",
    "            #if there is at least one fluent to swap                    \n",
    "            if len(present_base_goal_fluents) > 0:\n",
    "                \n",
    "                #builds list of fluents that are in the goal state but not in the base goal state, and don't introduce inconsistencies\n",
    "                candidates_list = []\n",
    "                for fluent in non_base_goal_fluents:\n",
    "                    if check_if_fluent_is_usable(fluent, goal_state):\n",
    "                        candidates_list.append(fluent)\n",
    "                \n",
    "                #if there is at least one fluent to swap\n",
    "                if len(candidates_list) > 0:\n",
    "                    random_fluent = random.choice(candidates_list)\n",
    "\n",
    "                    fluent_to_swap = random.choice(present_base_goal_fluents)\n",
    "                    # before = check_consistency(goal_state) #debug\n",
    "                    goal_state[goal_state.index(fluent_to_swap)] = random_fluent\n",
    "                    # after = check_consistency(goal_state) #debug\n",
    "                    # if before == True and after == False: #debug\n",
    "                    #     print(f\"|<| Goal state is not consistent: {goal_state}\\n\\t base_goal_state: {base_goal_state},\\n\\t fluent_to_swap: {fluent_to_swap},\\n\\t random_fluent: {random_fluent}\\n\\n\") #debug\n",
    "                    \n",
    "        #compute the recognizability after the step\n",
    "        new_recognizability = compute_recognizability(base_goal_state, [base_goal_state] + goal_state_list)\n",
    "        \n",
    "        #if we have not changed the recognizability, the randomness patience is reduced\n",
    "        #we will try another random goal state in the next iteration \n",
    "        if new_recognizability == running_recognizability:\n",
    "            randomness_patience -= 1\n",
    "            \n",
    "            #if we have hit too many times stuck goal states, we will regenerate the last we encountered, so the one in this iteration\n",
    "            if randomness_patience == 0 and regeneration_patience > 0:\n",
    "                regeneration_patience -= 1\n",
    "                randomness_patience = randomness_patience_constant\n",
    "                                \n",
    "                # Patience reached: regenerate stuck goal_state\n",
    "                new_goal_state = generate_goal_state(package_for_goal_set, pos_for_goal_set, number_of_goals)\n",
    "                \n",
    "                # Store the regeneration (the key is the recognizability of the prevoius configuration)\n",
    "                goal_regeneration_dict[running_recognizability] = goal_state_list\n",
    "                \n",
    "                # Replace the goal state in the list with the new one\n",
    "                goal_state = new_goal_state\n",
    "            \n",
    "            #if we have exhausted the regeneration patience, we will stop trying and take the best configuration we have\n",
    "            elif randomness_patience == 0 and regeneration_patience == 0:\n",
    "                # Patience exhausted; if we have any regenerations, choose the one with recognizability closest to target.\n",
    "                if goal_regeneration_dict:\n",
    "                    # Find the closest recognizability to the target\n",
    "                    # must use midpoint as we could heve rec_scores both above and below the target\n",
    "                    target_recognizability = (rec_target[0] + rec_target[1]) / 2 \n",
    "                    closest_recognizability = min(goal_regeneration_dict.keys(), key=lambda x: abs(x - target_recognizability))\n",
    "                    running_recognizability = closest_recognizability\n",
    "                    goal_state_list = goal_regeneration_dict[closest_recognizability]\n",
    "                print(f\"Patience exhausted, breaking: Best recognizability: {running_recognizability} | Target: {rec_target}\") # debug\n",
    "\n",
    "                # to keep track of errors we do the distace to the target range\n",
    "                error = abs(running_recognizability - rec_target[0]) if running_recognizability < rec_target[0] else abs(running_recognizability - rec_target[1])\n",
    "                running_sum_rec_error += error\n",
    "                global_counter += 1\n",
    "                break\n",
    "        #print(f\"Running recognizability at end of step: {running_recognizability}\")\n",
    "        running_recognizability = new_recognizability\n",
    "    return base_goal_state, goal_state_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd9b679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# #check_same_object_in_fluents testing\n",
    "# fluent1 = \"at obj1 pos1\"\n",
    "# fluent2 = \"at obj2 pos2\"\n",
    "\n",
    "# print(check_same_object_in_fluents(fluent1, fluent2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "79c334c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patience exhausted, breaking: Best recognizability: 0.1201 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1278 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.125 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.2708 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.2708 | Target: [0.1, 0.2]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1111 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1104 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.2222 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.2076 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1056 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1007 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.2319 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.2076 | Target: [0.1, 0.2]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1201 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1201 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1931 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1104 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1104 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1833 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1201 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1181 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1007 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1347 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1833 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1007 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1104 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1201 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1104 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.125 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1104 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1007 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1201 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1979 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1201 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1493 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.2951 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1201 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1347 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1104 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.125 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.159 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1056 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1104 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.3535 | Target: [0.1, 0.2]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1056 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1278 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1007 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1347 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1104 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.3292 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1347 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1347 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1007 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1104 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.2951 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.2076 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1347 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1347 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1104 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1007 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1104 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1104 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1201 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1035 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1181 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.159 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1035 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1007 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.159 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1201 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1056 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1104 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1687 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1007 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1201 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.2076 | Target: [0.1, 0.2]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1208 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.2708 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1104 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1201 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1104 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1056 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1347 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1007 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1104 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.159 | Target: [0, 0.1]\n",
      "Patience exhausted, breaking: Best recognizability: 0.1042 | Target: [0, 0.1]\n",
      "Global counter: 93\n",
      "Running sum of recognizability error: 3.4775000000000005\n",
      "Average recognizability generation error: 0.037392473118279576\n",
      "Average recognizability generation error on whole dataset: 0.0005795833333333334\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for plan in plans:\n",
    "    if test:\n",
    "        if count >= 3:\n",
    "            break\n",
    "    elif count > plans_to_process:\n",
    "        break\n",
    "    #begin plan processing\n",
    "        \n",
    "    all_obj_set = set()\n",
    "    package_for_goal_set = set()\n",
    "    pos_for_goal_set = set()\n",
    "    \n",
    "    #* find all objects in the initial state and actions\n",
    "    for line in plan.initial_state:\n",
    "        for obj in line.split(\" \")[1:]:\n",
    "            all_obj_set.add(obj)\n",
    "    for action in plan.actions:\n",
    "        for fluent in action.positiveEffects:\n",
    "            for obj in fluent.split(\" \")[1:]:\n",
    "                all_obj_set.add(obj)\n",
    "        for fluent in action.negativeEffects:\n",
    "            for obj in fluent.split(\" \")[1:]:\n",
    "                all_obj_set.add(obj)\n",
    "        for fluent in action.precondition:\n",
    "            for obj in fluent.split(\" \")[1:]:\n",
    "                all_obj_set.add(obj)\n",
    "    \n",
    "    #split the objects in their types\n",
    "    pos_set = set()\n",
    "    apn_set = set()\n",
    "    cit_set = set()\n",
    "    apt_set = set()\n",
    "    tru_set = set()\n",
    "    pack_set = set()\n",
    "    obj_set_dict = {}\n",
    "    for obj in all_obj_set:\n",
    "        if obj.startswith(\"pos\"):\n",
    "            pos_set.add(obj)\n",
    "            pos_for_goal_set.add(obj) #these will be used for goal creation\n",
    "        elif obj.startswith(\"obj\"):\n",
    "            pack_set.add(obj)\n",
    "            package_for_goal_set.add(obj) #these will be used for goal creation\n",
    "        elif obj.startswith(\"apn\"):\n",
    "            apn_set.add(obj)\n",
    "        elif obj.startswith(\"cit\"):\n",
    "            cit_set.add(obj)\n",
    "        elif obj.startswith(\"tru\"):\n",
    "            tru_set.add(obj)\n",
    "        elif obj.startswith(\"apt\"):\n",
    "            apt_set.add(obj)\n",
    "    \n",
    "    #useful for printing the plan\n",
    "    if len(pos_set) > 0:\n",
    "        obj_set_dict[\"location\"] = pos_set\n",
    "    if len(apn_set) > 0:\n",
    "        obj_set_dict[\"airplane\"] = apn_set\n",
    "    if len(cit_set) > 0:\n",
    "        obj_set_dict[\"city\"] = cit_set\n",
    "    if len(apt_set) > 0:\n",
    "        obj_set_dict[\"airport\"] = apt_set\n",
    "    if len(tru_set) > 0:\n",
    "        obj_set_dict[\"truck\"] = tru_set\n",
    "    if len(pack_set) > 0:\n",
    "        obj_set_dict[\"package\"] = pack_set\n",
    "    \n",
    "    # raise an exception if number of goals > number of packages or positions\n",
    "    # as we won't be able to generate a goal state with the given number of goals\n",
    "    if number_of_goals > len(package_for_goal_set):\n",
    "        raise exception(f\"Number of goals {number_of_goals} is greater than the number of objects {len(package_for_goal_set)}\")\n",
    "\n",
    "    # in case we want that objects can not be at the same position\n",
    "    # if number_of_goals > len(pos_for_goal_set):\n",
    "    #     raise exception(f\"Number of goals {number_of_goals} is greater than the number of positions {len(pos_for_goal_set)}\")\n",
    "\n",
    "    write_original_plan(plan=plan, goal_state=plan.goals, obj_set_dict=obj_set_dict)\n",
    "    \n",
    "    for interval in rec_classes:    \n",
    "        goal_state_list = create_goal_state_list(package_for_goal_set=package_for_goal_set, \n",
    "                                                 pos_for_goal_set=pos_for_goal_set, \n",
    "                                                 number_of_goals=number_of_goals, \n",
    "                                                 versions_per_plan=versions_per_plan)\n",
    "        \n",
    "        # for goal_state in goal_state_list: # debug\n",
    "        #     #check if any goal state has conflicting fluents # debug\n",
    "        #     for fluent1 in goal_state: # debug\n",
    "        #         for fluent2 in goal_state: # debug\n",
    "        #             if fluent1 != fluent2 and check_same_object_in_fluents(fluent1, fluent2): # debug\n",
    "        #                 raise exception(f\"Goal state {goal_state} has conflicting fluents: {fluent1} and {fluent2}\") # debug\n",
    "        \n",
    "        #print(f\"Goals state list before: {goal_state_list}\") # debug\n",
    "        base_goal_state, adapted_goal_state_list = adapt_goal_state_list_to_recognizability(base_goal_state=goal_state_list[0], goal_state_list=goal_state_list[1:], \n",
    "                                                                                            package_for_goal_set=package_for_goal_set, pos_for_goal_set=pos_for_goal_set, \n",
    "                                                                                            number_of_goals=number_of_goals, rec_target=interval, \n",
    "                                                                                            randomness_patience=10, regeneration_patience=5)\n",
    "        #print(f\"Goals state list after adapt: {[base_goal_state] + adapted_goal_state_list}\") # debug\n",
    "\n",
    "        write_and_save_versions(plan=plan, goal_state_list=adapted_goal_state_list, obj_set_dict=obj_set_dict, rec_class=interval)\n",
    "\n",
    "    count = count + 1\n",
    "    \n",
    "print(\"Global counter:\", global_counter)\n",
    "print(\"Running sum of recognizability error:\", running_sum_rec_error)\n",
    "print(\"Average recognizability generation error:\", running_sum_rec_error/global_counter if global_counter > 0 else 0)\n",
    "print(\"Average recognizability generation error on whole dataset:\", running_sum_rec_error/(plans_to_process*versions_per_plan))\n",
    "\n",
    "global_counter = 0\n",
    "\n",
    "running_sum_rec_error = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36b2314",
   "metadata": {},
   "source": [
    "most errors are for the lowest classes (e.g [0, 0.1]), not always a very low rec can be easily reached.\n",
    "average error is about 0.04, but looking at the whole dataset its very low and not really a problem, most problems are in their classes, i think that making the lowest class [0, 0.2] is a good compromise, reducing errors to just 0.5% instances, with the same avg error, and lower when looking at whole dataset\n",
    "\n",
    "We have to decide dataset output format, now i make a dir for every problem: this contains subdirs for every recognizability class which contain the versions generated. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444c2bcd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goal_rec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
