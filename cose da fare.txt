prendere i file test da quel file e tenerli da parte gli _og

tenere la struttura per i risultati anche quelli originali
usando lo script get results and predictions

usare i miei piani tutti quelli ho con augment
vedere dove arriva il modello, se è decente vediamo di andare a togliere dal train quelli che passano confidence e experience vengono tolti dal train e finche num size prossima batch non passa confidence()

nel fare i result con get pred and result aggiungere campo per recognizability

da fast downward prendere anche problemi che non hanno tutte le versioni





sembra che anche mettendo 0, 0.001 non aumenti particolarmente l'uso, può essere che non trova una precision per un certo goal?

se si mette 0, 0 ovviamente usa sempre solo sistema 1 quindi le threshold non sono rotte in se.


lavorare sul non mettere in train le cose che passano
 lavorare per batch (ricordarsi di arrivare a dimensione giusta per l'incremento)

modificare threshold

fare un analisi statistica su valore predizioni giuste e valore predizioni sbagliate
    - perchè non prendere un top k? non sappiamo k immagino perchè nel domain il numero di fluenti in un goal cambia
    - threshold dinamiche? fare un analisi sul vettore della predizione e mettere threshold in base a distribuzione
    


provando a tweakkare la threshold si osserva il trend generale dove l'accuracy non sembra cambiare particolarmente finche non si scende sotto i 0.2 di approx (ovvero se > 0.2 then 1 else 0)

ho provato per estremo a vedere 0.05 come threshold aspettandomi il finimondo e seppur non sia bello, sembra dire che la rete abbia abbastanza imparato a tenere molto vicino a 0 le cose che devono essere 0 (questo ha senso pensando che la trainiamo con binary cross entropy e quindi viene penalizzata molto quando si discosta dai valori giusti, essendo la maggior parte 0, sta imparando prima a tenere a zero le cose che devono essere a zero e poi piano piano sta alzando le cose che devono essere a 1).

Probabilmente quindi dandogli ancora altri piani dovrebbe migliorare in quella direzione





Riguardo adaptive incremental training,
Fasi principali:
    - carica i dict vari
    - inizializza il modello
    - divide in remaining_plans(listone di tutti i piani che rimangono), passed_plans(passano MC) e trained_plans(usati per training)
    - parte il ciclo di train adaptive
        [- opzionalmente si selezionano un tot di piani di review (ovvero che hanno superato il MC in passato, da riguardare se passano ancora, se non passano, si tirano nella lista di piani che non hanno passato e verrano usati per train)]
        - scorro remaining_plans e mando in eval piani, prendo le predizioni, le raccolgo e faccio come MC finchè non arrivo ad un tot di piani "falliti" per fare una batch
        - aggiungo i piani vecchi a questi secondo old_plans_factor
        - traino il modello per questa experience
        - salvo i piani che ho usato per trainare
        - ripeto finchè non finisco remaining_plans (non ne ho abbastanza per fare un'experience)

